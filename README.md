# QSTAR 

## Database

We first provides a new Viewport-based Omnidirectional Video Quality Assessment (VOD-VQA) database, which includes eighteen raw viewport videos  covering the salient regions of omnidirectional video (ODV) contents, and corresponding 774 impaired samples generated by compressing the raw content using a variety of combinations of its Spatial (frame size, s), Temporal (frame rate, t), and Amplitude (quantization step-size, q) Resolutions (STAR). Total 160 subjects have participated to assess the processed viewport videos rendered on the head mounted display (HMD) when they stabilize their fixations to the salient areas. 

### Download:

[MOS data]( http://yun.nju.edu.cn/f/afc0dbce1b/ )

[Test Viewport videos]( http://yun.nju.edu.cn/f/354aa9169e/ )

[Validation Viewport videos](  http://yun.nju.edu.cn/f/f393696b3d/ )

## Model

### Quality Model for Viewport Video

We have developed an analytical model  $Q^{VP}_{STAR}$ to connect the perceptual quality of a compressed viewport video with its STAR variables. All four model parameters can be linearly predicted using extracted content features, making the proposed metric generalized to various contents.  This model correlates well with the mean opinion scores (MOSs) of viewport videos. 

### Quality Model for ODV

The viewport-based quality model can be easily extended to infer the overall ODV quality by linearly weighing the saliency-aggregated qualities of salient viewports and the quality of quick-scanning (or non-salient) area. Experiments have shown that inferred model $Q^{ODV}_{STAR}$ correlates with the collected MOS very well with competitive performance to the state-of-the-art algorithm, across another four independent and third-party ODV assessment datasets, including [IPP_IVQD]( https://ieeexplore.ieee.org/document/8350375 ), [IVQAD2017]( https://ieeexplore.ieee.org/document/7965610 ), [ISTOmnidirectional]( https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10752/107520P/Subjective-and-objective-quality-assessment-of-omnidirectional-video/10.1117/12.2321679.short?SSO=1 ) and [VQA-ODV]( https://github.com/Archer-Tatsu/VQA-ODV ).

## Related Scripts  

We have uploaded the relevant scripts for the  $Q^{ODV}_{STAR}$ calculation. The main function is ``QSTAR_ODV_com.m``.

###  **Notes** 

- Make sure you have FFmpeg environment variables in your system.
- We used the saliency prediction method in ["Saliency in VR: How Do People Explore Virtual Environments?"]( https://vsitzmann.github.io/vr-saliency/ ). Other methods for ODV saliency prediction are also effective, which will have an impact on the accuracy of the final result .
- The input ODV size for the above scripts is 3840x1920. The corresponding parameters in ``QSTAR_ODV_com.m`` and ``viewport_extract.m``.